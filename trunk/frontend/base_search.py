# Copyright 2009 Google Inc.  All Rights Reserved.
#

import cgi
import datetime
import os
import urllib
import logging

from google.appengine.api import users
from google.appengine.api import urlfetch
from google.appengine.api import memcache
from google.appengine.ext import webapp
from google.appengine.ext.webapp import template
from google.appengine.ext.webapp.util import run_wsgi_app
from google.appengine.ext import db
from xml.dom import minidom

## Use urlfetch instead of httplib
#gdata.service.http_request_handler = gdata.urlfetch

import geocode
import models
import searchresult
import utils

RESULT_CACHE_TIME = 900 # seconds


# Google base namespace, typically xmlns:g='http://base.google.com/ns/1.0'
XMLNS_BASE='http://base.google.com/ns/1.0'
# Atom namespace, typically xmlns='http://www.w3.org/2005/Atom'
XMLNS_ATOM='http://www.w3.org/2005/Atom'

def make_base_arg(x):
  return "base_" + x

# note: many of the XSS and injection-attack defenses are unnecessary
# given that the callers are also protecting us, but I figure better
# safe than sorry, and defense-in-depth.
def search(args):
  base_query = ""

  # TODO: injection attack on q
  if "q" in args:
    base_query += ' '+args["q"]

  # TODO: injection attacks in vol_loc
  if args["vol_loc"] != "":
    args["vol_dist"] = int(str(args["vol_dist"]))
    base_query += ' [location: @"%s" + %dmi]' % (args["vol_loc"],args["vol_dist"])

  # Base URL for snipets search on Base.
  #   Docs: http://code.google.com/apis/base/docs/2.0/attrs-queries.html
  # TODO: injection attack on backend
  if "backend" not in args:
    args["backends"] = "http://www.google.com/base/feeds/snippets"

  if make_base_arg("customer") not in args:
    args[make_base_arg("customer")] = 5663714;
  base_query += ' [customer id: '+str(int(args[make_base_arg("customer")]))+']'

  base_query += ' [detailurl]'

  if "num" not in args:
    args["num"] = 10
  if "start" not in args:
    args["start"] = 1

  #for k in args: logging.info("arg["+str(k)+"]="+str(args[k]))
  url_params = urllib.urlencode({'max-results': args["num"],
                                 'start-index': args["start"],
                                 'bq': base_query,
                                 'content': 'geocodes',
                                 })
  query_url = '%s?%s' % (args["backends"], url_params)

  return query(query_url, args, True)

def query(query_url, args, cache):
  result_set = searchresult.SearchResultSet(urllib.unquote(query_url),
                                            query_url,
                                            [])
  result_set.args = args

  memcache_key = 'query:' + query_url
  result_content = memcache.get('query:' + query_url)
  if not result_content:
    fetch_result = urlfetch.fetch(query_url)
    if fetch_result.status_code != 200:
      return result_set
    result_content = fetch_result.content
    if cache:
      memcache.set(memcache_key, result_content, time=RESULT_CACHE_TIME)

  dom = minidom.parseString(result_content)

  for i,entry in enumerate(dom.getElementsByTagName('entry')):
    # Note: using entry.getElementsByTagName('link')[0] isn't very stable;
    # consider iterating through them for the one where rel='alternate' or
    # whatever the right thing is.
    #urltag = entry.getElementsByTagName('link')[0].getAttribute('href')
    url = utils.GetXmlElementText(entry, XMLNS_BASE, 'detailurl')
    # ID is the 'stable id' of the item generated by base.
    # Note that this is not the base url expressed as the Atom id element.
    id = utils.GetXmlElementText(entry, XMLNS_BASE, 'id')
    # Base URL is the url of the item in base, expressed with the Atom id tag.
    base_url = utils.GetXmlElementText(entry, XMLNS_ATOM, 'id')
    snippet = utils.GetXmlElementText(entry, XMLNS_ATOM, 'content')
    title = utils.GetXmlElementText(entry, XMLNS_ATOM, 'title')
    location = utils.GetXmlElementText(entry, XMLNS_BASE, 'location_string')
    logging.info("title="+title+"  location="+str(location)+"  url="+url)
    res = searchresult.SearchResult(url, title, snippet, location, id, base_url)
    res.idx = i+1
    lat_element = entry.getElementsByTagName('g:latitude')
    long_element = entry.getElementsByTagName('g:longitude')
    res.latlong = ""
    if lat_element and long_element:
      res.latlong = utils.GetXmlDomText(lat_element[0]) + "," + \
          utils.GetXmlDomText(long_element[0])
    if res.latlong == ",":
      res.latlong = ""
    result_set.results.append(res)
    if cache and res.id:
      key = "searchresult:" + res.id
      memcache.set(key, res, time=RESULT_CACHE_TIME)
      # Datastore updates are expensive and space-consuming, so only add them
      # if a user expresses interest in an opportunity--e.g. elsewhere.

  return result_set


def get_from_id(id):
  """Return a searchresult from the stable ID."""
  key = 'searchresult:' + id
  try:
    search_result = memcache.get(key)
    if search_result:
      return search_result
  except Exception:
    search_result = None
  
  # Now things get more complex: We need to find the base entry from the
  # datastore, then look that up in base, then return that info.
  key = 'id:' + id
  info = models.VolunteerOpportunity.get_by_key_name(key)
  if not info:
    logging.warning('Could not find entry in datastore for id: %s' % id)
    return None
  if not info.base_url:
    logging.warning('Could not find base_url in datastore for id: %s' % id)
    return None
  result_set = query(info.base_url, None, True)
  if not result_set.results:
    # The base URL may have changed from under us. Oh well.
    logging.warning('Did not get results from base. id: %s base_url: %s '
                    'Last update: %s Previous failure: %s' %
                    (id, info.base_url, info.last_base_url_update, 
                     info.last_base_url_update_failure))
    info.base_url_failure_count += 1
    info.last_base_url_update_failure = datetime.datetime.now()
    info.put()
    return None
  
  if (result_set.results[0].id != id):
    logging.error('First result is not expected result. '
                  'Expected: %s Found: %s. len(results): %s' %
                  (id, result_set.results[0].id, len(results)))
    # Not sure if we should touch the VolunteerOpportunity or not.
    return None
  
  return result_set.results[0]
  
def get_from_ids(ids):
  """Return a result set containing multiple results for multiple ids."""
  result_set = searchresult.SearchResultSet('', '', [])
  for id in ids:
    result = get_from_id(id)
    if result:
      result_set.results.append(result)
  return result_set
